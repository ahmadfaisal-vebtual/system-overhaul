# Revenue Scanner: Major Pain Points and Recommended Fixes

This document identifies the major pain points in the revenue-scanner codebase that impact latency, robustness, and accuracy. Issues are ordered from easiest to hardest to implement, with clear explanations of why they matter and how to fix them.

---

## Table of Contents

1. [Easy Fixes (Low Effort, High Impact)](#easy-fixes)
2. [Medium Complexity Fixes](#medium-complexity-fixes)
3. [Advanced Fixes (Higher Effort, Significant Impact)](#advanced-fixes)

---

## Easy Fixes

### 1. Add Response Caching for LLM Calls

**Why This Matters:**
- Currently, every request triggers fresh LLM analysis, even for identical inputs
- LLM API calls are expensive (both in cost and latency - typically 2-10 seconds per call)
- The README explicitly states "No caching is implemented"
- Multiple modules may request similar data (e.g., company profile, industry classification)

**Current State:**
- No caching layer exists
- Each `/supplier-analysis` and `/prospect-analysis` call makes 10+ LLM API calls
- Identical URLs processed multiple times result in duplicate API calls

**Proposed Fix:**
Implement a simple in-memory cache using `cachetools` (already in requirements.txt) with TTL-based expiration:

```python
from cachetools import TTLCache
from functools import wraps
import hashlib
import json

# Cache for LLM responses (1 hour TTL, max 1000 entries)
llm_cache = TTLCache(maxsize=1000, ttl=3600)

def cache_llm_call(cache_key_func):
    """Decorator to cache LLM API calls"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Generate cache key from function name and arguments
            key = f"{func.__name__}:{cache_key_func(*args, **kwargs)}"
            if key in llm_cache:
                return llm_cache[key]
            result = func(*args, **kwargs)
            llm_cache[key] = result
            return result
        return wrapper
    return decorator
```

**Why This Works Better:**
- Reduces API costs by 50-80% for repeated requests
- Cuts latency from 15-30s to <1s for cached responses
- `cachetools` is already a dependency, so no new packages needed
- TTL ensures data freshness while allowing reuse

**Implementation Complexity:** ‚≠ê (Very Easy - 2-4 hours)

---

### 2. Replace `time.sleep()` with Async Alternatives

**Why This Matters:**
- `time.sleep()` blocks the entire thread, preventing other operations from running
- Found in retry logic across multiple files (`supplier_shared_discovery.py`, `cluster_mapping.py`, `categories_discovery.py`)
- In async contexts, this blocks the event loop and reduces concurrency

**Current State:**
```python
# supplier/supplier_shared_discovery.py:121
time.sleep(wait)  # Blocks thread during retries

# prospect/cluster_mapping.py:227
time.sleep(2)  # Blocks during retry backoff
```

**Proposed Fix:**
Replace blocking sleeps with `asyncio.sleep()` in async functions, or use `await` in async contexts:

```python
# For async functions
await asyncio.sleep(wait)

# For sync functions that need to yield, use threading.Event or asyncio.run()
# Or better: make the entire function async
```

**Why This Works Better:**
- Allows other coroutines to run during wait periods
- Improves overall system throughput
- Better resource utilization in async FastAPI endpoints

**Implementation Complexity:** ‚≠ê (Very Easy - 1-2 hours)

---

### 3. Add HTTP Connection Pooling

**Why This Matters:**
- Currently using `requests.get()` which creates new TCP connections for each request
- Connection establishment overhead: ~50-200ms per request
- Multiple modules fetch from the same domains (company websites, LinkedIn, Google APIs)
- No connection reuse = wasted time and resources

**Current State:**
```python
# supplier/supplier_shared_discovery.py:113
resp = requests.get(url, headers=headers, timeout=timeout)
# New connection created every time
```

**Proposed Fix:**
Use a `requests.Session()` with connection pooling:

```python
# Create a global session with connection pooling
_http_session = requests.Session()
_http_session.mount('https://', requests.adapters.HTTPAdapter(
    pool_connections=10,
    pool_maxsize=20,
    max_retries=3
))

def fetch_html(url: str, ...) -> Optional[str]:
    resp = _http_session.get(url, headers=headers, timeout=timeout)
    # Reuses existing connections
```

**Why This Works Better:**
- Reduces connection overhead by 50-200ms per request
- Better resource utilization (fewer file descriptors)
- Automatic retry handling via HTTPAdapter
- Can save 1-3 seconds per analysis run with multiple HTTP calls

**Implementation Complexity:** ‚≠ê (Very Easy - 1 hour)

---

### 4. Implement Structured Logging

**Why This Matters:**
- Currently using `print()` statements scattered throughout codebase
- No log levels, timestamps, or structured data
- Difficult to debug production issues
- No correlation between related log entries
- Can't filter or search logs effectively

**Current State:**
```python
# Found throughout codebase
print(f"üì° Fetching HTML: {url}")
print(f"‚ö†Ô∏è Fetch failed: {e}")
print(f"‚úÖ Completed supplier run")
```

**Proposed Fix:**
Use Python's `logging` module with structured formatting:

```python
import logging
import json
from datetime import datetime

# Configure structured logger
logger = logging.getLogger(__name__)
handler = logging.StreamHandler()
formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.setLevel(logging.INFO)

# Replace prints with:
logger.info(f"Fetching HTML: {url}", extra={"url": url, "attempt": attempt})
logger.error(f"Fetch failed: {e}", exc_info=True)
```

**Why This Works Better:**
- Enables log aggregation tools (ELK, Splunk, CloudWatch)
- Better debugging with timestamps and context
- Can filter by log level in production
- Structured data enables analytics and monitoring

**Implementation Complexity:** ‚≠ê‚≠ê (Easy - 3-4 hours)

---

### 5. Add Request Timeout Configuration

**Why This Matters:**
- Hardcoded timeouts scattered throughout codebase (15s, 12s, etc.)
- No way to adjust timeouts for different environments
- Slow external APIs can hang requests indefinitely
- Different endpoints may need different timeout strategies

**Current State:**
```python
# Multiple hardcoded timeouts
timeout=15  # supplier_shared_discovery.py:108
timeout=12  # emp.py:1079
timeout=12  # Multiple locations
```

**Proposed Fix:**
Centralize timeout configuration:

```python
# config.py
class TimeoutConfig:
    HTTP_REQUEST = int(os.getenv("HTTP_TIMEOUT", "15"))
    LLM_API = int(os.getenv("LLM_TIMEOUT", "60"))
    GOOGLE_API = int(os.getenv("GOOGLE_API_TIMEOUT", "10"))

# Usage
resp = requests.get(url, timeout=TimeoutConfig.HTTP_REQUEST)
```

**Why This Works Better:**
- Environment-specific tuning (dev vs prod)
- Easy to adjust without code changes
- Prevents indefinite hangs
- Better error handling with known timeout values

**Implementation Complexity:** ‚≠ê (Very Easy - 1 hour)

---

### 6. Add Input Validation and Sanitization

**Why This Matters:**
- URLs may be malformed, leading to wasted API calls
- No validation of country codes, currency codes
- Invalid inputs cause cryptic errors downstream
- Security risk: potential injection via URLs

**Current State:**
```python
# main.py:39
class URLRequest(BaseModel):
    url: str  # No validation
```

**Proposed Fix:**
Add Pydantic validators (Pydantic already in requirements):

```python
from pydantic import BaseModel, HttpUrl, validator
from urllib.parse import urlparse

class URLRequest(BaseModel):
    url: HttpUrl  # Validates URL format
    
    @validator('url')
    def normalize_url(cls, v):
        # Normalize to ensure consistency
        parsed = urlparse(str(v))
        return f"{parsed.scheme}://{parsed.netloc.lower()}"
    
    @validator('country')
    def validate_country_code(cls, v):
        # Validate ISO country codes
        valid_countries = ['US', 'NZ', 'AU', ...]  # Or use pycountry
        if v.upper() not in valid_countries:
            raise ValueError(f"Invalid country code: {v}")
        return v.upper()
```

**Why This Works Better:**
- Catches errors early (before expensive API calls)
- Consistent URL normalization reduces cache misses
- Better error messages for API consumers
- Prevents security issues

**Implementation Complexity:** ‚≠ê‚≠ê (Easy - 2-3 hours)

---

## Medium Complexity Fixes

### 7. Implement Async HTTP Requests

**Why This Matters:**
- Currently using synchronous `requests` library
- Blocks threads during I/O operations
- FastAPI is async-native, but most code is sync
- Multiple HTTP calls could run concurrently but run sequentially

**Current State:**
```python
# supplier/supplier_shared_discovery.py
resp = requests.get(url, ...)  # Blocks thread
html = fetch_html(url1)  # Waits
html2 = fetch_html(url2)  # Waits for url1 to finish
```

**Proposed Fix:**
Use `httpx` (already in requirements.txt) for async HTTP:

```python
import httpx

async def fetch_html_async(url: str, ...) -> Optional[str]:
    async with httpx.AsyncClient(timeout=timeout) as client:
        resp = await client.get(url, headers=headers)
        return resp.text

# Run multiple fetches concurrently
async def fetch_multiple(urls: List[str]):
    async with httpx.AsyncClient() as client:
        tasks = [fetch_html_async(url) for url in urls]
        results = await asyncio.gather(*tasks)
        return results
```

**Why This Works Better:**
- 3-5x faster for multiple HTTP calls (run in parallel)
- Better resource utilization (no thread blocking)
- Native async/await support in FastAPI
- Can reduce total HTTP time from 5-10s to 2-3s

**Implementation Complexity:** ‚≠ê‚≠ê‚≠ê (Medium - 1-2 days)

---

### 8. Add Exponential Backoff Retry Logic

**Why This Matters:**
- Current retry logic uses fixed delays (`time.sleep(2)`)
- Doesn't handle rate limits gracefully
- May overwhelm APIs during retries
- No jitter = thundering herd problem

**Current State:**
```python
# prospect/cluster_mapping.py:227
time.sleep(2)  # Fixed delay, no backoff
```

**Proposed Fix:**
Use `tenacity` library (already in requirements.txt):

```python
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((requests.RequestException, TimeoutError))
)
def fetch_with_retry(url: str) -> Optional[str]:
    resp = requests.get(url, timeout=15)
    resp.raise_for_status()
    return resp.text
```

**Why This Works Better:**
- Exponential backoff reduces API load
- Handles transient failures gracefully
- Configurable retry strategies
- Built-in jitter prevents synchronized retries

**Implementation Complexity:** ‚≠ê‚≠ê (Easy - 2-3 hours)

---

### 9. Implement Request Deduplication

**Why This Matters:**
- Same URLs may be fetched multiple times in a single request
- Multiple modules may request the same company data
- Wastes API quota and increases latency
- Cache misses due to URL normalization inconsistencies

**Current State:**
```python
# Multiple modules fetch company website independently
# supplier_shared_discovery.py fetches homepage
# supplier_value_prop.py fetches homepage again
# prospect_profile_discovery.py fetches homepage again
```

**Proposed Fix:**
Add a request-level deduplication cache:

```python
from functools import lru_cache
import asyncio

class RequestDeduplicator:
    def __init__(self):
        self._pending = {}
        self._lock = asyncio.Lock()
    
    async def get_or_fetch(self, key: str, fetch_func):
        """If same key is requested concurrently, reuse the pending request"""
        async with self._lock:
            if key in self._pending:
                # Wait for existing request
                return await self._pending[key]
            
            # Create new request
            future = asyncio.create_task(fetch_func())
            self._pending[key] = future
        
        try:
            result = await future
            return result
        finally:
            async with self._lock:
                self._pending.pop(key, None)
```

**Why This Works Better:**
- Eliminates duplicate fetches within same request
- Reduces API calls by 20-30%
- Faster response times
- Better API quota management

**Implementation Complexity:** ‚≠ê‚≠ê‚≠ê (Medium - 1 day)

---

### 10. Add Circuit Breaker Pattern

**Why This Matters:**
- External API failures can cascade and overwhelm the system
- No protection against repeated failures
- Wastes resources retrying failed endpoints
- Can cause timeouts and poor user experience

**Current State:**
- No circuit breaker implementation
- Retries continue even when API is down
- No fallback mechanisms

**Proposed Fix:**
Use `circuitbreaker` library or implement simple circuit breaker:

```python
from circuitbreaker import circuit

@circuit(failure_threshold=5, recovery_timeout=60)
def call_external_api(url: str):
    """Automatically opens circuit after 5 failures"""
    resp = requests.get(url, timeout=10)
    resp.raise_for_status()
    return resp.json()

# Or use pybreaker for more control
from pybreaker import CircuitBreaker

api_breaker = CircuitBreaker(fail_max=5, timeout_duration=60)

@api_breaker
def call_api():
    # Protected function
    pass
```

**Why This Works Better:**
- Prevents cascading failures
- Fast failure for known-broken endpoints
- Automatic recovery when service is restored
- Better user experience (fail fast vs timeout)

**Implementation Complexity:** ‚≠ê‚≠ê‚≠ê (Medium - 1 day)

---

### 11. Implement Health Check Endpoint

**Why This Matters:**
- No way to verify system health
- Load balancers can't route traffic intelligently
- Difficult to diagnose issues in production
- No readiness/liveness probes for Kubernetes

**Current State:**
- No health check endpoint exists
- Can't verify API connectivity
- No dependency health checks

**Proposed Fix:**
Add FastAPI health check endpoints:

```python
@app.get("/health")
async def health_check():
    """Basic health check"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.get("/health/ready")
async def readiness_check():
    """Check if service is ready to accept traffic"""
    checks = {
        "openai_api": check_openai_connection(),
        "google_api": check_google_connection(),
        "cache": check_cache_health()
    }
    all_healthy = all(checks.values())
    status_code = 200 if all_healthy else 503
    return JSONResponse(
        content={"status": "ready" if all_healthy else "not ready", "checks": checks},
        status_code=status_code
    )
```

**Why This Works Better:**
- Enables proper load balancer configuration
- Kubernetes liveness/readiness probes
- Better observability
- Faster incident detection

**Implementation Complexity:** ‚≠ê‚≠ê (Easy - 2-3 hours)

---

### 12. Add Metrics and Monitoring

**Why This Matters:**
- No visibility into system performance
- Can't identify bottlenecks
- No alerting on errors or latency
- Difficult to optimize without data

**Current State:**
- No metrics collection
- Execution times logged but not aggregated
- No error rate tracking
- No latency percentiles

**Proposed Fix:**
Add Prometheus metrics (or use existing `sentry-sdk`):

```python
from prometheus_client import Counter, Histogram, Gauge
import time

# Define metrics
request_count = Counter('requests_total', 'Total requests', ['endpoint', 'status'])
request_duration = Histogram('request_duration_seconds', 'Request duration')
llm_calls = Counter('llm_calls_total', 'LLM API calls', ['model', 'status'])
cache_hits = Counter('cache_hits_total', 'Cache hits', ['cache_type'])

@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    duration = time.time() - start_time
    request_duration.observe(duration)
    request_count.labels(
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    return response
```

**Why This Works Better:**
- Identify performance bottlenecks
- Track error rates and latency
- Enable alerting on thresholds
- Data-driven optimization

**Implementation Complexity:** ‚≠ê‚≠ê‚≠ê (Medium - 2-3 days)

---

### 13. Optimize ThreadPoolExecutor Usage

**Why This Matters:**
- Multiple `ThreadPoolExecutor` instances with varying `max_workers`
- No coordination between executors
- May create too many threads (context switching overhead)
- Async FastAPI endpoints using sync executors = suboptimal

**Current State:**
```python
# main.py:156
with ThreadPoolExecutor(max_workers=10) as executor:

# main.py:775
with ThreadPoolExecutor(max_workers=5) as executor:

# supplier/supplier_clusters.py:1323
with ThreadPoolExecutor(max_workers=num_clusters * 3) as executor:
```

**Proposed Fix:**
Use async/await instead of ThreadPoolExecutor where possible:

```python
# Instead of:
with ThreadPoolExecutor(max_workers=10) as executor:
    future = executor.submit(sync_function, arg)
    result = future.result()

# Use:
result = await asyncio.to_thread(sync_function, arg)

# Or better: make functions async
async def async_function(arg):
    # Native async implementation
    pass
```

**Why This Works Better:**
- Better resource utilization
- No thread pool overhead
- Native async/await is more efficient
- Easier to reason about concurrency

**Implementation Complexity:** ‚≠ê‚≠ê‚≠ê‚≠ê (Medium-Hard - 3-5 days)

---

## Advanced Fixes

### 14. Implement Persistent Caching (Redis)

**Why This Matters:**
- In-memory cache is lost on restart
- Can't share cache across multiple instances
- No cache persistence = wasted API calls on restarts
- Redis infrastructure already exists (see `ARM/revenue-scanner.bicep`)

**Current State:**
- Redis is provisioned but not used
- Only in-memory caching (if any)
- Cache lost on deployment/restart

**Proposed Fix:**
Use Redis for distributed caching:

```python
import redis
import json
from typing import Optional

redis_client = redis.Redis(
    host=os.getenv("REDIS_HOST"),
    port=int(os.getenv("REDIS_PORT", 6379)),
    decode_responses=True
)

def cache_llm_response(key: str, value: Any, ttl: int = 3600):
    """Cache LLM response in Redis"""
    redis_client.setex(
        key,
        ttl,
        json.dumps(value, default=str)
    )

def get_cached_response(key: str) -> Optional[Any]:
    """Retrieve cached response from Redis"""
    cached = redis_client.get(key)
    if cached:
        return json.loads(cached)
    return None
```

**Why This Works Better:**
- Cache survives restarts
- Shared across multiple instances
- Better cache hit rates
- Can implement cache warming strategies

**Implementation Complexity:** ‚≠ê‚≠ê‚≠ê‚≠ê (Medium-Hard - 2-3 days)

---

### 15. Add Request Queuing and Rate Limiting

**Why This Matters:**
- No protection against request spikes
- Can overwhelm LLM APIs (rate limits)
- No prioritization of requests
- Poor user experience during high load

**Current State:**
- All requests processed immediately
- No queuing mechanism
- No rate limiting

**Proposed Fix:**
Implement request queuing with rate limiting:

```python
from asyncio import Queue
from datetime import datetime, timedelta
import asyncio

class RateLimiter:
    def __init__(self, max_requests: int, time_window: int):
        self.max_requests = max_requests
        self.time_window = timedelta(seconds=time_window)
        self.requests = []
        self.lock = asyncio.Lock()
    
    async def acquire(self):
        async with self.lock:
            now = datetime.now()
            # Remove old requests
            self.requests = [r for r in self.requests if now - r < self.time_window]
            
            if len(self.requests) >= self.max_requests:
                # Wait until oldest request expires
                wait_time = (self.requests[0] + self.time_window - now).total_seconds()
                await asyncio.sleep(wait_time)
                return await self.acquire()
            
            self.requests.append(now)

# Use in endpoints
llm_rate_limiter = RateLimiter(max_requests=10, time_window=60)

@app.post("/supplier-analysis")
async def supplier_analysis(...):
    await llm_rate_limiter.acquire()  # Wait if rate limit exceeded
    # Process request
```

**Why This Works Better:**
- Prevents API rate limit errors
- Better resource management
- Can implement priority queues
- Smoother user experience

**Implementation Complexity:** ‚≠ê‚≠ê‚≠ê‚≠ê (Medium-Hard - 2-3 days)

---

### 16. Implement Database for Results Storage

**Why This Matters:**
- No persistence of analysis results
- Can't query historical data
- No analytics on trends
- Results lost after request completes

**Current State:**
- Results only returned in API response
- No database layer
- SQLAlchemy in requirements but not used

**Proposed Fix:**
Add PostgreSQL/MySQL for result storage:

```python
from sqlalchemy import create_engine, Column, String, JSON, DateTime
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

Base = declarative_base()

class SupplierAnalysis(Base):
    __tablename__ = 'supplier_analyses'
    
    id = Column(String, primary_key=True)
    url = Column(String, index=True)
    country = Column(String)
    result = Column(JSON)
    created_at = Column(DateTime)
    execution_time = Column(Float)

# Store results after analysis
def store_supplier_analysis(url: str, country: str, result: dict):
    analysis = SupplierAnalysis(
        id=generate_id(),
        url=url,
        country=country,
        result=result,
        created_at=datetime.now()
    )
    session.add(analysis)
    session.commit()
```

**Why This Works Better:**
- Historical data for analytics
- Can build dashboards
- Query past analyses
- Enable A/B testing of algorithms

**Implementation Complexity:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Hard - 1 week)

---

### 17. Add Request Batching for LLM Calls

**Why This Matters:**
- Multiple similar LLM calls could be batched
- Reduces API round trips
- Lower latency for batch operations
- Better API quota utilization

**Current State:**
- Each LLM call is independent
- No batching mechanism
- Sequential processing of similar requests

**Proposed Fix:**
Implement batching for similar LLM requests:

```python
from collections import defaultdict
import asyncio

class LLMBatcher:
    def __init__(self, batch_size: int = 5, batch_timeout: float = 0.5):
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.pending = defaultdict(list)
        self.lock = asyncio.Lock()
    
    async def batch_call(self, prompt_type: str, prompt: str):
        """Add request to batch and return when batch is processed"""
        future = asyncio.Future()
        
        async with self.lock:
            self.pending[prompt_type].append((prompt, future))
            
            if len(self.pending[prompt_type]) >= self.batch_size:
                # Process batch immediately
                batch = self.pending[prompt_type]
                self.pending[prompt_type] = []
                asyncio.create_task(self._process_batch(prompt_type, batch))
        
        # Wait for result
        return await future
    
    async def _process_batch(self, prompt_type: str, batch: List):
        # Make single batched API call
        results = await openai_client.batch_create(...)
        # Distribute results to futures
        for (prompt, future), result in zip(batch, results):
            future.set_result(result)
```

**Why This Works Better:**
- Reduces API calls by 50-80% for similar requests
- Lower latency (one round trip vs many)
- Better API quota usage
- Cost savings

**Implementation Complexity:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Hard - 1 week)

---

### 18. Implement Graceful Shutdown

**Why This Matters:**
- No handling of SIGTERM/SIGINT
- In-flight requests may be lost on shutdown
- No cleanup of resources
- Poor deployment experience

**Current State:**
- No shutdown handlers
- Requests may be interrupted
- No resource cleanup

**Proposed Fix:**
Add graceful shutdown handling:

```python
import signal
import asyncio
from contextlib import asynccontextmanager

shutdown_event = asyncio.Event()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    yield
    # Shutdown
    shutdown_event.set()
    # Wait for in-flight requests
    await asyncio.sleep(5)  # Or use proper request tracking

app = FastAPI(lifespan=lifespan)

def signal_handler(signum, frame):
    shutdown_event.set()

signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)

@app.middleware("http")
async def check_shutdown(request: Request, call_next):
    if shutdown_event.is_set():
        return JSONResponse(
            content={"error": "Server shutting down"},
            status_code=503
        )
    return await call_next(request)
```

**Why This Works Better:**
- Prevents request loss during deployments
- Clean resource cleanup
- Better user experience
- Production-ready behavior

**Implementation Complexity:** ‚≠ê‚≠ê‚≠ê (Medium - 1 day)

---

## Summary

### Quick Wins (Implement First)
1. Response caching for LLM calls
2. Replace `time.sleep()` with async alternatives
3. HTTP connection pooling
4. Structured logging
5. Request timeout configuration
6. Input validation

### Medium-Term Improvements
7. Async HTTP requests
8. Exponential backoff retries
9. Request deduplication
10. Circuit breaker pattern
11. Health check endpoints
12. Metrics and monitoring
13. Optimize ThreadPoolExecutor usage

### Long-Term Enhancements
14. Persistent caching (Redis)
15. Request queuing and rate limiting
16. Database for results storage
17. LLM request batching
18. Graceful shutdown

---

## Expected Impact Summary

| Fix | Latency Reduction | Robustness Improvement | Accuracy Impact | Effort |
|-----|------------------|----------------------|-----------------|--------|
| LLM Caching | 50-80% (cached) | High (fewer API failures) | None | ‚≠ê |
| Async HTTP | 30-50% | Medium | None | ‚≠ê‚≠ê‚≠ê |
| Connection Pooling | 10-20% | Medium | None | ‚≠ê |
| Retry Logic | 0% | High | Medium | ‚≠ê‚≠ê |
| Circuit Breaker | 0% | High | None | ‚≠ê‚≠ê‚≠ê |
| Request Dedup | 20-30% | Low | None | ‚≠ê‚≠ê‚≠ê |
| Health Checks | 0% | High | None | ‚≠ê‚≠ê |
| Metrics | 0% | High | Low (data-driven) | ‚≠ê‚≠ê‚≠ê |

---

## Implementation Priority Recommendation

**Phase 1 (Week 1):** Quick Wins
- Implement fixes #1-6 (caching, async sleep, connection pooling, logging, timeouts, validation)

**Phase 2 (Week 2-3):** Medium Complexity
- Implement fixes #7-13 (async HTTP, retries, dedup, circuit breaker, health checks, metrics, thread optimization)

**Phase 3 (Month 2):** Advanced Features
- Implement fixes #14-18 (Redis, queuing, database, batching, graceful shutdown)

This phased approach ensures immediate improvements while building toward a more robust, scalable system.


# Logical Improvements & SOTA LLM Engineering Patterns

Now, we'll outline the algorithmic improvements, workflow orchestration optimizations, context engineering enhancements, and state-of-the-art LLM software engineering patterns that can significantly improve latency, accuracy, and robustness.

---

## Table of Contents

1. [Workflow Orchestration Improvements](#workflow-orchestration-improvements)
2. [Context Engineering & Prompt Optimization](#context-engineering--prompt-optimization)
3. [SOTA LLM Software Engineering Patterns](#sota-llm-software-engineering-patterns)
4. [Algorithmic Efficiency Improvements](#algorithmic-efficiency-improvements)
5. [Data Flow & State Management](#data-flow--state-management)

---

## Workflow Orchestration Improvements

### 1. Implement Dynamic Dependency Graph Execution

**Current Problem:**
- Fixed execution phases (Phase 1, Phase 2, Phase 3) with hardcoded dependencies
- Modules wait unnecessarily for dependencies that may not be needed
- No conditional execution based on intermediate results
- Cannot skip expensive operations when simpler alternatives succeed

**Current State:**
```python
# main.py:156 - Fixed phase execution
# PHASE 1: Start all independent tasks
# PHASE 2: Process Phase 1 results and start dependent tasks
# PHASE 3: Process Phase 2 results
```

**Actionable Steps:**
1. Build a dependency graph where each module declares its inputs and outputs
2. Use a DAG (Directed Acyclic Graph) execution engine (e.g., `airflow`, `prefect`, or custom)
3. Enable dynamic task scheduling: start tasks as soon as their dependencies are met
4. Implement conditional execution: skip expensive paths when simpler ones succeed
5. Add early exit strategies: if industry classification fails, skip dependent modules gracefully
6. Implement result caching at workflow level: cache intermediate results to avoid recomputation

**Why This Works Better:**
- Reduces total execution time by 20-40% through better parallelization
- Enables adaptive workflows that adjust based on data quality
- Better resource utilization (no idle waiting)
- Easier to add new modules without restructuring phases

---

### 2. Implement Workflow State Persistence

**Current Problem:**
- No state persistence between workflow steps
- If a step fails, entire workflow restarts from beginning
- Cannot resume partial executions
- No audit trail of what was computed when

**Actionable Steps:**
1. Store workflow state in Redis or database after each step completes
2. Implement checkpoint/resume mechanism: save state after each module
3. Add workflow ID tracking: each analysis gets a unique workflow ID
4. Store intermediate results: cache module outputs with workflow context
5. Implement retry with state: on failure, resume from last successful checkpoint
6. Add workflow versioning: track which version of each module was used

**Why This Works Better:**
- Enables partial recovery (don't lose all work on one failure)
- Supports debugging (can inspect intermediate states)
- Allows workflow replay for testing
- Better observability of execution paths

---

### 3. Implement Conditional Module Execution

**Current Problem:**
- All modules always execute regardless of data quality or requirements
- No way to skip expensive operations when simpler alternatives work
- Cannot adapt workflow based on prospect characteristics (e.g., skip cluster mapping if no supplier profile)

**Actionable Steps:**
1. Add execution conditions to each module (e.g., "only run if supplier_profile exists")
2. Implement rule-based routing: use decision trees to determine which modules to run
3. Add quality gates: skip downstream modules if upstream quality is too low
4. Implement fallback chains: try expensive method, fall back to simpler if it fails
5. Add feature flags: enable/disable modules based on configuration
6. Use confidence thresholds: only run expensive validation if confidence is below threshold

**Why This Works Better:**
- Reduces unnecessary API calls by 30-50%
- Faster execution for simple cases
- Better cost control (skip expensive LLM calls when not needed)
- More robust error handling

---

### 4. Implement Parallel Data Fetching with Smart Batching

**Current Problem:**
- URL discovery uses sequential Google searches
- Multiple modules fetch the same URLs independently
- No batching of similar requests
- HTTP requests not optimized for parallel execution

**Actionable Steps:**
1. Batch Google Search API calls: combine multiple queries into single requests where possible
2. Implement URL fetch deduplication: track which URLs are being fetched and share results
3. Use async HTTP client with connection pooling for all external requests
4. Pre-fetch common pages: fetch homepage, about, contact in parallel at workflow start
5. Implement request queue: batch similar requests (e.g., all "about" page fetches together)
6. Add fetch result sharing: modules subscribe to URL fetch events instead of fetching independently

**Why This Works Better:**
- Reduces HTTP overhead by 40-60%
- Faster URL discovery (parallel vs sequential)
- Better API quota management
- Lower latency for multi-page analysis

---

## Context Engineering & Prompt Optimization

### 5. Implement Few-Shot Learning in Prompts

**Current Problem:**
- Prompts are instruction-heavy with no examples
- Models must infer format from instructions alone
- Inconsistent outputs require post-processing
- No demonstration of desired output quality

**Actionable Steps:**
1. Add 2-3 high-quality examples to each prompt showing input ‚Üí output
2. Use diverse examples: cover edge cases (small companies, unusual industries, etc.)
3. Include negative examples: show what NOT to do
4. Store examples in separate files for easy A/B testing
5. Rotate examples periodically to prevent overfitting to specific patterns
6. Use example selection: choose examples most similar to current input

**Why This Works Better:**
- Improves output consistency by 30-50%
- Reduces need for post-processing
- Better format adherence (models learn from examples)
- Fewer validation errors

---

### 6. Implement Prompt Versioning and A/B Testing

**Current Problem:**
- Prompts are hardcoded in source code
- No way to test prompt variations
- Cannot measure prompt performance
- Changes require code deployment

**Actionable Steps:**
1. Extract all prompts to external files (YAML or JSON)
2. Implement prompt versioning: each prompt has a version number
3. Add A/B testing framework: route requests to different prompt versions
4. Track metrics per prompt version: accuracy, latency, cost
5. Implement gradual rollout: start with 10% traffic to new prompt
6. Add prompt comparison dashboard: visualize performance differences
7. Store prompt metadata: author, date, performance metrics

**Why This Works Better:**
- Enables data-driven prompt optimization
- Faster iteration (no code changes needed)
- Can roll back bad prompts instantly
- Better understanding of what works

---

### 7. Implement Context Compression and Summarization

**Current Problem:**
- Full webpage HTML passed to LLM (very long context)
- Multiple pages concatenated without summarization
- Context windows filled with irrelevant information
- High token costs for large contexts

**Actionable Steps:**
1. Extract only relevant sections from webpages (use BeautifulSoup to get main content)
2. Implement content summarization: use smaller model to summarize pages before passing to main model
3. Use semantic chunking: split large documents into semantically meaningful chunks
4. Implement relevance filtering: score content chunks and keep only top-K most relevant
5. Use embedding-based retrieval: find most relevant sections using vector similarity
6. Add context length limits: truncate or summarize if context exceeds threshold
7. Implement hierarchical summarization: summarize summaries for very long documents

**Why This Works Better:**
- Reduces token costs by 40-70%
- Faster LLM responses (less to process)
- Better focus on relevant information
- Can handle larger documents without hitting context limits

---

### 8. Implement Structured Context Passing

**Current Problem:**
- Context passed as long text strings
- No structured data format
- LLM must parse unstructured text
- Difficult to validate context quality

**Actionable Steps:**
1. Convert context to structured JSON format before passing to LLM
2. Use schema-based context: define Pydantic models for context structure
3. Implement context validation: ensure required fields are present
4. Add context metadata: include confidence scores, source URLs, timestamps
5. Use function calling for structured data: pass context as function parameters
6. Implement context templates: reusable context structures for common patterns
7. Add context compression: use abbreviations and references for repeated data

**Why This Works Better:**
- More reliable parsing (structured vs free text)
- Better context quality control
- Easier debugging (can inspect structured context)
- Lower token usage (structured is more compact)

---

### 9. Implement Multi-Turn Conversation Patterns

**Current Problem:**
- Single-shot LLM calls for complex tasks
- No iterative refinement
- Cannot ask follow-up questions
- No way to correct errors mid-conversation

**Actionable Steps:**
1. Break complex tasks into multi-turn conversations
2. Implement iterative refinement: ask LLM to improve output based on validation errors
3. Add clarification requests: if output is ambiguous, ask for clarification
4. Use chain-of-thought: ask LLM to show reasoning before final answer
5. Implement self-correction: ask LLM to review and correct its own output
6. Add feedback loops: use validation results to improve next iteration
7. Store conversation history: maintain context across turns

**Why This Works Better:**
- Higher accuracy through iterative refinement
- Better error recovery (can fix mistakes)
- More explainable outputs (see reasoning)
- Handles ambiguous inputs better

---

### 10. Implement Prompt Templates with Dynamic Variables

**Current Problem:**
- Prompts are string concatenations
- No template system
- Difficult to maintain consistent formatting
- No validation of prompt completeness

**Actionable Steps:**
1. Use Jinja2 or similar templating engine for prompts
2. Define prompt templates with typed variables
3. Implement template validation: ensure all variables are filled
4. Add template inheritance: base templates with specialized variants
5. Store templates in version control: track prompt changes
6. Add template testing: unit tests for prompt generation
7. Implement template caching: cache rendered prompts for same inputs

**Why This Works Better:**
- Easier prompt maintenance
- Consistent formatting across all prompts
- Catches errors early (missing variables)
- Enables prompt reuse across modules

---

## SOTA LLM Software Engineering Patterns

### 11. Implement Function Calling / Tool Use Optimization

**Current Problem:**
- Using `web_search` tool but not optimally
- No custom tools for internal operations
- LLM must infer when to use tools
- Tool results not cached or reused

**Actionable Steps:**
1. Create custom tools for common operations (e.g., `fetch_company_page`, `search_linkedin`)
2. Implement tool result caching: cache tool outputs with TTL
3. Add tool selection guidance: provide examples of when to use each tool
4. Implement tool chaining: allow tools to call other tools
5. Add tool result validation: validate tool outputs before passing to LLM
6. Use tool streaming: stream tool results as they become available
7. Implement tool fallbacks: if one tool fails, try alternative

**Why This Works Better:**
- More reliable data fetching (tools vs free-form web search)
- Better cost control (cached tool results)
- Faster execution (parallel tool execution)
- More accurate results (validated tool outputs)

---

### 12. Implement Chain-of-Thought Reasoning

**Current Problem:**
- LLM outputs final answers directly
- No visibility into reasoning process
- Difficult to debug incorrect outputs
- No confidence indication

**Actionable Steps:**
1. Add reasoning steps to prompts: ask LLM to show its thinking
2. Implement step-by-step reasoning: break complex tasks into reasoning steps
3. Add reasoning validation: check if reasoning is sound
4. Store reasoning traces: log reasoning for analysis
5. Use reasoning for confidence scoring: longer/more detailed reasoning = higher confidence
6. Implement reasoning-based error detection: flag outputs with weak reasoning
7. Add reasoning summarization: condense reasoning for storage

**Why This Works Better:**
- Higher accuracy (models perform better with reasoning)
- Better debugging (can see where reasoning went wrong)
- More explainable outputs
- Enables confidence scoring

---

### 13. Implement Self-Consistency and Ensemble Methods

**Current Problem:**
- Single LLM call per task
- No validation of output consistency
- No way to detect hallucinations
- Single point of failure

**Actionable Steps:**
1. Run same prompt multiple times with different temperatures
2. Implement majority voting: use most common answer across runs
3. Add consistency checking: flag outputs that vary significantly
4. Use ensemble of models: combine outputs from different models
5. Implement confidence aggregation: average confidence scores
6. Add disagreement detection: if outputs disagree, request clarification
7. Store all outputs: keep all attempts for analysis

**Why This Works Better:**
- Higher accuracy (ensemble > single model)
- Better error detection (inconsistency = potential error)
- More robust (less affected by model quirks)
- Provides confidence estimates

---

### 14. Implement Output Validation and Self-Correction

**Current Problem:**
- Only Pydantic validation (schema validation)
- No semantic validation
- No self-correction mechanism
- Errors require manual intervention

**Actionable Steps:**
1. Add semantic validation rules: check if output makes sense (e.g., RPE values in reasonable range)
2. Implement self-correction: if validation fails, ask LLM to correct output
3. Add cross-validation: check consistency with other module outputs
4. Implement confidence-based validation: only validate low-confidence outputs thoroughly
5. Use validation feedback loops: use validation errors to improve prompts
6. Add validation metrics: track validation failure rates
7. Implement automatic retry: retry with corrected prompt on validation failure

**Why This Works Better:**
- Catches errors before they propagate
- Reduces manual intervention
- Improves output quality over time
- Better error recovery

---

### 15. Implement Model Routing and Tiered Execution

**Current Problem:**
- Using same model (GPT-5) for all tasks
- Expensive models used for simple tasks
- No model selection based on task complexity
- Cannot leverage faster/cheaper models

**Actionable Steps:**
1. Classify tasks by complexity: simple (classification) vs complex (analysis)
2. Route simple tasks to faster models (GPT-4o-mini, Claude Haiku)
3. Use expensive models only for complex reasoning tasks
4. Implement model fallback: if cheap model fails, retry with expensive model
5. Add model performance tracking: measure accuracy/cost per model per task
6. Implement dynamic routing: choose model based on input characteristics
7. Use model ensembles: combine outputs from different models

**Why This Works Better:**
- Reduces costs by 50-70% (cheaper models for simple tasks)
- Faster execution (faster models for simple tasks)
- Better resource utilization
- Maintains quality (expensive models for complex tasks)

---

### 16. Implement Streaming and Progressive Enhancement

**Current Problem:**
- Wait for complete LLM response before processing
- No partial results available
- Long wait times for complex tasks
- No way to show progress

**Actionable Steps:**
1. Use streaming API for LLM calls: process tokens as they arrive
2. Implement progressive output: show partial results as they become available
3. Add streaming aggregation: combine streaming outputs from multiple calls
4. Implement early stopping: if confidence is high enough, stop early
5. Use streaming for long outputs: stream large JSON responses
6. Add progress indicators: show % complete for long operations
7. Implement streaming validation: validate partial outputs as they stream

**Why This Works Better:**
- Better user experience (see progress)
- Faster perceived latency
- Can cancel long operations early
- More responsive system

---

### 17. Implement Embedding-Based Semantic Search

**Current Problem:**
- Keyword-based URL discovery (Google Search)
- No semantic understanding of content
- May miss relevant pages with different terminology
- Relies on exact keyword matches

**Actionable Steps:**
1. Generate embeddings for webpage content using OpenAI embeddings API
2. Store embeddings in vector database (Pinecone, Weaviate, or pgvector)
3. Implement semantic search: find pages by meaning, not keywords
4. Add hybrid search: combine keyword and semantic search
5. Use embeddings for content similarity: find similar companies/products
6. Implement embedding-based clustering: group similar prospects automatically
7. Add embedding caching: cache embeddings for frequently accessed pages

**Why This Works Better:**
- Finds relevant content even with different terminology
- Better accuracy (semantic understanding vs keyword matching)
- Enables similarity-based recommendations
- More robust to language variations

---

### 18. Implement RAG (Retrieval-Augmented Generation)

**Current Problem:**
- LLM relies on web search for each request
- No knowledge base of past analyses
- Cannot learn from historical data
- Repeated analysis of same companies

**Actionable Steps:**
1. Build knowledge base: store past analysis results in vector database
2. Implement retrieval: find similar past analyses before running new analysis
3. Use retrieved context: pass relevant past analyses to LLM as context
4. Add knowledge base updates: continuously update with new analyses
5. Implement relevance filtering: only retrieve highly relevant past analyses
6. Use RAG for common queries: answer from knowledge base when possible
7. Add knowledge base search: allow users to search past analyses

**Why This Works Better:**
- Faster responses (retrieve vs compute)
- More consistent (learns from past)
- Lower costs (fewer API calls)
- Better accuracy (more context)

---

## Algorithmic Efficiency Improvements

### 19. Replace Google Search with Sitemap Parsing

**Current Problem:**
- Using Google Search API to find company pages (expensive, rate-limited)
- Sequential searches for each page type (about, products, services, contact)
- May miss pages not indexed by Google
- Slow (multiple API calls)

**Actionable Steps:**
1. Parse sitemap.xml first: extract all URLs from company sitemap
2. Filter sitemap URLs by keywords: match URLs to page types (about, products, etc.)
3. Use Google Search as fallback: only if sitemap parsing fails
4. Implement sitemap caching: cache parsed sitemaps with TTL
5. Add sitemap validation: check sitemap freshness and completeness
6. Combine sitemap + homepage links: merge sitemap URLs with homepage navigation
7. Implement smart URL scoring: score URLs from sitemap using same logic as Google results

**Why This Works Better:**
- 10-20x faster (local parsing vs API calls)
- More complete (gets all pages, not just indexed ones)
- Lower cost (no API calls)
- More reliable (no rate limits)

---

### 20. Implement Unified Scoring Algorithm

**Current Problem:**
- Multiple scoring functions for different purposes (URL scoring, relevance scoring, etc.)
- Inconsistent scoring logic across modules
- Difficult to tune and optimize
- No unified confidence scoring

**Actionable Steps:**
1. Create unified scoring framework: single scoring interface for all use cases
2. Implement configurable scoring weights: tune weights without code changes
3. Add scoring calibration: ensure scores are comparable across modules
4. Use machine learning for scoring: train model to predict relevance
5. Implement ensemble scoring: combine multiple scoring methods
6. Add scoring explainability: show why a score was assigned
7. Store scoring metadata: track score distributions and trends

**Why This Works Better:**
- Consistent scoring across system
- Easier to optimize (single place to tune)
- Better accuracy (ML-based scoring)
- More explainable results

---

### 21. Implement Early Exit Strategies

**Current Problem:**
- All modules always execute to completion
- No way to skip expensive operations when simpler ones succeed
- Cannot adapt based on intermediate results
- Wastes resources on low-value operations

**Actionable Steps:**
1. Add confidence thresholds: if confidence is high enough, skip validation
2. Implement quality gates: if upstream quality is low, skip downstream
3. Use cached results: if cached result exists and is fresh, skip computation
4. Add fast path detection: detect simple cases and use fast path
5. Implement timeout-based early exit: if operation takes too long, use fallback
6. Use approximate answers: if exact answer is expensive, use approximation
7. Add resource-based early exit: if system is overloaded, skip non-critical operations

**Why This Works Better:**
- Faster execution for simple cases
- Better resource utilization
- Lower costs (skip expensive operations)
- More responsive system

---

### 22. Implement Batch Processing for Similar Requests

**Current Problem:**
- Each request processed independently
- No batching of similar operations
- Cannot leverage batch API endpoints
- Wastes opportunities for optimization

**Actionable Steps:**
1. Identify batchable operations: find operations that can be batched (e.g., multiple URL fetches)
2. Implement request queuing: queue similar requests for batching
3. Use batch timeouts: wait for batch window before processing
4. Implement batch API calls: use batch endpoints where available
5. Add batch result distribution: distribute batch results to individual requests
6. Use batch optimization: optimize batch size for performance
7. Add batch metrics: track batch efficiency and savings

**Why This Works Better:**
- Lower API costs (batch discounts)
- Faster processing (parallel batch operations)
- Better API quota utilization
- More efficient resource usage

---

### 23. Implement Incremental Processing

**Current Problem:**
- Full recomputation on every request
- No way to update existing results incrementally
- Cannot leverage previous computations
- Wastes computation on unchanged data

**Actionable Steps:**
1. Track data freshness: store timestamps for all data sources
2. Implement change detection: detect when source data has changed
3. Use incremental updates: only recompute changed parts
4. Implement dependency tracking: track which outputs depend on which inputs
5. Add incremental validation: validate only changed parts
6. Use diff-based processing: compute only differences
7. Implement smart invalidation: invalidate only dependent results

**Why This Works Better:**
- Faster updates (only changed parts)
- Lower costs (fewer API calls)
- Better resource utilization
- More responsive to changes

---

## Data Flow & State Management

### 24. Implement Data Lineage Tracking

**Current Problem:**
- No tracking of data flow through system
- Cannot trace where results came from
- Difficult to debug incorrect outputs
- No audit trail

**Actionable Steps:**
1. Add lineage metadata to all data: track source, transformation, timestamp
2. Implement lineage graph: build graph of data dependencies
3. Add lineage queries: query "where did this value come from?"
4. Implement lineage visualization: visualize data flow
5. Store lineage in database: persist lineage for analysis
6. Add lineage validation: check lineage consistency
7. Use lineage for debugging: trace errors to source

**Why This Works Better:**
- Better debugging (can trace errors)
- More explainable (see data provenance)
- Enables data quality tracking
- Supports compliance/auditing

---

### 25. Implement Intermediate Result Storage

**Current Problem:**
- Intermediate results not stored
- Cannot reuse partial computations
- Difficult to debug (no intermediate states)
- No way to resume failed operations

**Actionable Steps:**
1. Store all intermediate results: save outputs from each module
2. Implement result versioning: version intermediate results
3. Add result metadata: store confidence, timestamp, source
4. Use result caching: cache intermediate results with TTL
5. Implement result querying: query past intermediate results
6. Add result comparison: compare results across runs
7. Use results for analytics: analyze intermediate result quality

**Why This Works Better:**
- Enables partial recovery
- Better debugging (inspect intermediate states)
- Supports analytics
- Faster development (reuse results)

---

### 26. Implement Data Validation Pipeline

**Current Problem:**
- Validation happens only at final output
- Errors propagate through system
- No early error detection
- Difficult to identify root cause

**Actionable Steps:**
1. Add validation at each step: validate inputs and outputs of each module
2. Implement validation rules: define rules for each data type
3. Use validation feedback: use validation errors to improve upstream
4. Add validation metrics: track validation failure rates
5. Implement validation caching: cache validation results
6. Use validation for routing: route based on validation results
7. Add validation explainability: explain why validation failed

**Why This Works Better:**
- Catches errors early
- Prevents error propagation
- Better error messages
- Improves data quality over time

---

### 27. Implement State Machine for Workflow Execution

**Current Problem:**
- Workflow state managed ad-hoc
- No clear state transitions
- Difficult to reason about workflow state
- No state recovery mechanism

**Actionable Steps:**
1. Define workflow state machine: states (pending, running, completed, failed)
2. Implement state transitions: define valid transitions
3. Add state persistence: store state in database
4. Implement state recovery: recover from saved state
5. Add state validation: validate state transitions
6. Use state for monitoring: track workflow progress
7. Implement state-based routing: route based on current state

**Why This Works Better:**
- Clearer workflow logic
- Better error recovery
- Easier to reason about
- More robust execution

---

## Summary & Priority

### High Priority (Immediate Impact)
1. **Sitemap Parsing** (#19) - 10-20x faster URL discovery
2. **Few-Shot Learning** (#5) - 30-50% better consistency
3. **Context Compression** (#7) - 40-70% cost reduction
4. **Model Routing** (#15) - 50-70% cost reduction
5. **Early Exit Strategies** (#21) - Faster execution

### Medium Priority (Significant Impact)
6. **Dynamic Dependency Graph** (#1) - 20-40% faster execution
7. **Workflow State Persistence** (#2) - Better reliability
8. **Prompt Versioning** (#6) - Data-driven optimization
9. **Function Calling Optimization** (#11) - Better reliability
10. **Self-Consistency** (#13) - Higher accuracy

### Long-Term (Strategic Value)
11. **RAG Implementation** (#18) - Knowledge base benefits
12. **Embedding-Based Search** (#17) - Better accuracy
13. **Data Lineage Tracking** (#24) - Better observability
14. **Incremental Processing** (#23) - Efficiency gains
15. **State Machine** (#27) - Better architecture

---

## Implementation Roadmap

**Phase 1 (Month 1): Quick Wins**
- Implement sitemap parsing (#19)
- Add few-shot examples to prompts (#5)
- Implement context compression (#7)
- Add model routing (#15)

**Phase 2 (Month 2): Workflow Improvements**
- Build dynamic dependency graph (#1)
- Implement workflow state persistence (#2)
- Add conditional execution (#3)
- Implement early exit strategies (#21)

**Phase 3 (Month 3): LLM Patterns**
- Implement function calling optimization (#11)
- Add chain-of-thought reasoning (#12)
- Implement self-consistency (#13)
- Add output validation (#14)

**Phase 4 (Month 4+): Advanced Features**
- Implement RAG (#18)
- Add embedding-based search (#17)
- Implement data lineage (#24)
- Build state machine (#27)

This phased approach ensures immediate improvements while building toward a more sophisticated, efficient system.


